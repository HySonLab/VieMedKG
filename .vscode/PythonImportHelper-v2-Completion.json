[
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "HTTPBasicAuth",
        "importPath": "requests.auth",
        "description": "requests.auth",
        "isExtraImport": true,
        "detail": "requests.auth",
        "documentation": {}
    },
    {
        "label": "HTTPBasicAuth",
        "importPath": "requests.auth",
        "description": "requests.auth",
        "isExtraImport": true,
        "detail": "requests.auth",
        "documentation": {}
    },
    {
        "label": "HTTPBasicAuth",
        "importPath": "requests.auth",
        "description": "requests.auth",
        "isExtraImport": true,
        "detail": "requests.auth",
        "documentation": {}
    },
    {
        "label": "HTTPBasicAuth",
        "importPath": "requests.auth",
        "description": "requests.auth",
        "isExtraImport": true,
        "detail": "requests.auth",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "AzureChatOpenAI",
        "importPath": "langchain_community.chat_models",
        "description": "langchain_community.chat_models",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models",
        "documentation": {}
    },
    {
        "label": "AzureChatOpenAI",
        "importPath": "langchain_community.chat_models",
        "description": "langchain_community.chat_models",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models",
        "documentation": {}
    },
    {
        "label": "AzureChatOpenAI",
        "importPath": "langchain_community.chat_models",
        "description": "langchain_community.chat_models",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "sentence_bleu",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "SmoothingFunction",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "sentence_bleu",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "SmoothingFunction",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "sentence_bleu",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "SmoothingFunction",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "sentence_bleu",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "SmoothingFunction",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "importPath": "rouge",
        "description": "rouge",
        "isExtraImport": true,
        "detail": "rouge",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "importPath": "rouge",
        "description": "rouge",
        "isExtraImport": true,
        "detail": "rouge",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "importPath": "rouge",
        "description": "rouge",
        "isExtraImport": true,
        "detail": "rouge",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "importPath": "rouge",
        "description": "rouge",
        "isExtraImport": true,
        "detail": "rouge",
        "documentation": {}
    },
    {
        "label": "meteor_score",
        "importPath": "nltk.translate.meteor_score",
        "description": "nltk.translate.meteor_score",
        "isExtraImport": true,
        "detail": "nltk.translate.meteor_score",
        "documentation": {}
    },
    {
        "label": "meteor_score",
        "importPath": "nltk.translate.meteor_score",
        "description": "nltk.translate.meteor_score",
        "isExtraImport": true,
        "detail": "nltk.translate.meteor_score",
        "documentation": {}
    },
    {
        "label": "meteor_score",
        "importPath": "nltk.translate.meteor_score",
        "description": "nltk.translate.meteor_score",
        "isExtraImport": true,
        "detail": "nltk.translate.meteor_score",
        "documentation": {}
    },
    {
        "label": "meteor_score",
        "importPath": "nltk.translate.meteor_score",
        "description": "nltk.translate.meteor_score",
        "isExtraImport": true,
        "detail": "nltk.translate.meteor_score",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "HTTPAdapter",
        "importPath": "requests.adapters",
        "description": "requests.adapters",
        "isExtraImport": true,
        "detail": "requests.adapters",
        "documentation": {}
    },
    {
        "label": "HTTPAdapter",
        "importPath": "requests.adapters",
        "description": "requests.adapters",
        "isExtraImport": true,
        "detail": "requests.adapters",
        "documentation": {}
    },
    {
        "label": "HTTPAdapter",
        "importPath": "requests.adapters",
        "description": "requests.adapters",
        "isExtraImport": true,
        "detail": "requests.adapters",
        "documentation": {}
    },
    {
        "label": "HTTPAdapter",
        "importPath": "requests.adapters",
        "description": "requests.adapters",
        "isExtraImport": true,
        "detail": "requests.adapters",
        "documentation": {}
    },
    {
        "label": "Retry",
        "importPath": "requests.packages.urllib3.util.retry",
        "description": "requests.packages.urllib3.util.retry",
        "isExtraImport": true,
        "detail": "requests.packages.urllib3.util.retry",
        "documentation": {}
    },
    {
        "label": "Retry",
        "importPath": "requests.packages.urllib3.util.retry",
        "description": "requests.packages.urllib3.util.retry",
        "isExtraImport": true,
        "detail": "requests.packages.urllib3.util.retry",
        "documentation": {}
    },
    {
        "label": "Retry",
        "importPath": "requests.packages.urllib3.util.retry",
        "description": "requests.packages.urllib3.util.retry",
        "isExtraImport": true,
        "detail": "requests.packages.urllib3.util.retry",
        "documentation": {}
    },
    {
        "label": "Retry",
        "importPath": "requests.packages.urllib3.util.retry",
        "description": "requests.packages.urllib3.util.retry",
        "isExtraImport": true,
        "detail": "requests.packages.urllib3.util.retry",
        "documentation": {}
    },
    {
        "label": "GraphCypherQAChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "GraphCypherQAChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "Neo4jGraph",
        "importPath": "langchain_community.graphs",
        "description": "langchain_community.graphs",
        "isExtraImport": true,
        "detail": "langchain_community.graphs",
        "documentation": {}
    },
    {
        "label": "Neo4jGraph",
        "importPath": "langchain_community.graphs",
        "description": "langchain_community.graphs",
        "isExtraImport": true,
        "detail": "langchain_community.graphs",
        "documentation": {}
    },
    {
        "label": "ChatGoogleGenerativeAI",
        "importPath": "langchain_google_genai",
        "description": "langchain_google_genai",
        "isExtraImport": true,
        "detail": "langchain_google_genai",
        "documentation": {}
    },
    {
        "label": "GoogleGenerativeAIEmbeddings",
        "importPath": "langchain_google_genai",
        "description": "langchain_google_genai",
        "isExtraImport": true,
        "detail": "langchain_google_genai",
        "documentation": {}
    },
    {
        "label": "ChatGoogleGenerativeAI",
        "importPath": "langchain_google_genai",
        "description": "langchain_google_genai",
        "isExtraImport": true,
        "detail": "langchain_google_genai",
        "documentation": {}
    },
    {
        "label": "GoogleGenerativeAIEmbeddings",
        "importPath": "langchain_google_genai",
        "description": "langchain_google_genai",
        "isExtraImport": true,
        "detail": "langchain_google_genai",
        "documentation": {}
    },
    {
        "label": "ChatGoogleGenerativeAI",
        "importPath": "langchain_google_genai",
        "description": "langchain_google_genai",
        "isExtraImport": true,
        "detail": "langchain_google_genai",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "FewShotPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "FewShotPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "SemanticSimilarityExampleSelector",
        "importPath": "langchain_core.example_selectors",
        "description": "langchain_core.example_selectors",
        "isExtraImport": true,
        "detail": "langchain_core.example_selectors",
        "documentation": {}
    },
    {
        "label": "SemanticSimilarityExampleSelector",
        "importPath": "langchain_core.example_selectors",
        "description": "langchain_core.example_selectors",
        "isExtraImport": true,
        "detail": "langchain_core.example_selectors",
        "documentation": {}
    },
    {
        "label": "Neo4jVector",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "Neo4jVector",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "langchain",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain",
        "description": "langchain",
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "google.generativeai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "google.generativeai",
        "description": "google.generativeai",
        "detail": "google.generativeai",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "py2neo",
        "description": "py2neo",
        "isExtraImport": true,
        "detail": "py2neo",
        "documentation": {}
    },
    {
        "label": "Node",
        "importPath": "py2neo",
        "description": "py2neo",
        "isExtraImport": true,
        "detail": "py2neo",
        "documentation": {}
    },
    {
        "label": "Relationship",
        "importPath": "py2neo",
        "description": "py2neo",
        "isExtraImport": true,
        "detail": "py2neo",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "translators",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "translators",
        "description": "translators",
        "detail": "translators",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "get_scores",
        "kind": 2,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "def get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n    return bleu, rouge_score, meteor\nexample_prompt = PromptTemplate.from_template(",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "write_log_entry",
        "kind": 2,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "def write_log_entry(entry, file_path):\n    with open(file_path, \"a\") as f:\n        f.write(json.dumps(entry, ensure_ascii=False, indent=4) + \",\\n\")\ndef run(chain, question):\n    return chain.run(question)\ndef call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "def run(chain, question):\n    return chain.run(question)\ndef call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:\n            print(f\"Error: {e}. Retrying...\")\ndef get_gemini(text):",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "call_model_with_retry",
        "kind": 2,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "def call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:\n            print(f\"Error: {e}. Retrying...\")\ndef get_gemini(text):\n    genai.configure(api_key=google_api_key)\n    # Set up the model",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "get_gemini",
        "kind": 2,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "def get_gemini(text):\n    genai.configure(api_key=google_api_key)\n    # Set up the model\n    generation_config = {\n        \"temperature\": 0,\n        \"top_p\": 1,\n        \"top_k\": 1,\n        \"max_output_tokens\": 50000,\n    }\n    safety_settings = [",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "call_model_with_retry",
        "kind": 2,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "def call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:\n            print(f\"Error: {e}. Retrying...\")\nimport time\ngemini_inference_times = []\n# Process the first 20 samples",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "write_mean_scores",
        "kind": 2,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "def write_mean_scores(scores, file_path):\n    mean_scores = {\n        metric: sum(values) / len(values) for metric, values in scores.items()\n    }\n    with open(file_path, \"w\") as f:\n        for metric, score in mean_scores.items():\n            f.write(f\"{metric}: {score}\\n\")\nwrite_mean_scores(gemini_scores, gemini_results_path)\n# Write logs to JSON files\ndef write_log(log, file_path):",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "write_log",
        "kind": 2,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "def write_log(log, file_path):\n    with open(file_path, \"w\") as f:\n        json.dump(log, f, ensure_ascii=False, indent=4)\nwrite_log(gemini_log, gemini_log_path)",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "rouge",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "rouge = Rouge()\nload_dotenv(\"key.env\")\naccess_token = \"\"\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\nfrom langchain.chains import GraphCypherQAChain\nfrom langchain_community.graphs import Neo4jGraph\n# from langchain_openai import ChatOpenAI\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "access_token",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "access_token = \"\"\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\nfrom langchain.chains import GraphCypherQAChain\nfrom langchain_community.graphs import Neo4jGraph\n# from langchain_openai import ChatOpenAI\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "uri",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "uri = os.getenv(\"URI\")\nuser = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngoogle_api_key = os.getenv(\"GOOGLE_API_KEY\")\nembeddings = GoogleGenerativeAIEmbeddings(\n    model=\"models/embedding-001\",\n    google_api_key=google_api_key,\n)\ngraph = Neo4jGraph(url=uri, username=user, password=password)\nexamples = [",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "user",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "user = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngoogle_api_key = os.getenv(\"GOOGLE_API_KEY\")\nembeddings = GoogleGenerativeAIEmbeddings(\n    model=\"models/embedding-001\",\n    google_api_key=google_api_key,\n)\ngraph = Neo4jGraph(url=uri, username=user, password=password)\nexamples = [\n    {",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "password",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "password = os.getenv(\"PASSWORD\")\ngoogle_api_key = os.getenv(\"GOOGLE_API_KEY\")\nembeddings = GoogleGenerativeAIEmbeddings(\n    model=\"models/embedding-001\",\n    google_api_key=google_api_key,\n)\ngraph = Neo4jGraph(url=uri, username=user, password=password)\nexamples = [\n    {\n        \"question\": \"Phương pháp điều trị cho bệnh [U lympho sau phúc mạc] là gì?\",",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "google_api_key",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\nembeddings = GoogleGenerativeAIEmbeddings(\n    model=\"models/embedding-001\",\n    google_api_key=google_api_key,\n)\ngraph = Neo4jGraph(url=uri, username=user, password=password)\nexamples = [\n    {\n        \"question\": \"Phương pháp điều trị cho bệnh [U lympho sau phúc mạc] là gì?\",\n        \"query\": \"MATCH (b:ĐIỀU_TRỊ) WHERE b.tên_bệnh = 'U lympho sau phúc mạc' RETURN b\",",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "embeddings = GoogleGenerativeAIEmbeddings(\n    model=\"models/embedding-001\",\n    google_api_key=google_api_key,\n)\ngraph = Neo4jGraph(url=uri, username=user, password=password)\nexamples = [\n    {\n        \"question\": \"Phương pháp điều trị cho bệnh [U lympho sau phúc mạc] là gì?\",\n        \"query\": \"MATCH (b:ĐIỀU_TRỊ) WHERE b.tên_bệnh = 'U lympho sau phúc mạc' RETURN b\",\n    },",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "graph",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "graph = Neo4jGraph(url=uri, username=user, password=password)\nexamples = [\n    {\n        \"question\": \"Phương pháp điều trị cho bệnh [U lympho sau phúc mạc] là gì?\",\n        \"query\": \"MATCH (b:ĐIỀU_TRỊ) WHERE b.tên_bệnh = 'U lympho sau phúc mạc' RETURN b\",\n    },\n    {\n        \"question\": \"Nguyên nhân của bệnh [Chảy máu khoảng cách sau phúc mạc] là gì?\",\n        \"query\": \"MATCH (b:BỆNH) WHERE b.tên_bệnh = 'Chảy máu khoảng cách sau phúc mạc' RETURN b\",\n    },",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "examples",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "examples = [\n    {\n        \"question\": \"Phương pháp điều trị cho bệnh [U lympho sau phúc mạc] là gì?\",\n        \"query\": \"MATCH (b:ĐIỀU_TRỊ) WHERE b.tên_bệnh = 'U lympho sau phúc mạc' RETURN b\",\n    },\n    {\n        \"question\": \"Nguyên nhân của bệnh [Chảy máu khoảng cách sau phúc mạc] là gì?\",\n        \"query\": \"MATCH (b:BỆNH) WHERE b.tên_bệnh = 'Chảy máu khoảng cách sau phúc mạc' RETURN b\",\n    },\n    {",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "results_dir",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "results_dir = \"/Users/hoanganh692004/Desktop/products-knowledge-graph/results\"\nlogs_dir = \"/Users/hoanganh692004/Desktop/products-knowledge-graph/logs\"\ngemini_results_path = os.path.join(results_dir, \"gemini_graph_cypher.txt\")\ngemini_log_path = os.path.join(logs_dir, \"gemini_graph_cypher.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "logs_dir",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "logs_dir = \"/Users/hoanganh692004/Desktop/products-knowledge-graph/logs\"\ngemini_results_path = os.path.join(results_dir, \"gemini_graph_cypher.txt\")\ngemini_log_path = os.path.join(logs_dir, \"gemini_graph_cypher.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_results_path",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "gemini_results_path = os.path.join(results_dir, \"gemini_graph_cypher.txt\")\ngemini_log_path = os.path.join(logs_dir, \"gemini_graph_cypher.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_log_path",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "gemini_log_path = os.path.join(logs_dir, \"gemini_graph_cypher.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_scores",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "gemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_log",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "gemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "smoothing_function",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "smoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n    return bleu, rouge_score, meteor",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "example_prompt",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "example_prompt = PromptTemplate.from_template(\n    \"User input: {question}\\nCypher query: {query}\"\n)\nPREFIX = \"\"\"\n    I have a knowledge graph for Vietnamese traditional medicine, where each node represents a disease \"BỆNH\", \"ĐIỀU_TRỊ\", \"TRIỆU_CHỨNG\", \"THUỐC\", \"LỜI_KHUYÊN\". Each node can have the following properties:\n    1. BỆNH\n        - mô_tả_bệnh\n        - loại_bệnh\n        - tên_bệnh\n        - nguyên_nhân",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "PREFIX",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "PREFIX = \"\"\"\n    I have a knowledge graph for Vietnamese traditional medicine, where each node represents a disease \"BỆNH\", \"ĐIỀU_TRỊ\", \"TRIỆU_CHỨNG\", \"THUỐC\", \"LỜI_KHUYÊN\". Each node can have the following properties:\n    1. BỆNH\n        - mô_tả_bệnh\n        - loại_bệnh\n        - tên_bệnh\n        - nguyên_nhân\n    2. ĐIỀU_TRỊ\n        - khoa_điều_trị\n        - tỉ_lệ_chữa_khỏi",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "prompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=PREFIX,\n    suffix=\"User input: {question}\\nCypher query: \",\n    input_variables=[\"question\", \"schema\"],\n)\ngemini_chain = GraphCypherQAChain.from_llm(\n    ChatGoogleGenerativeAI(\n        model=\"gemini-pro\",",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_chain",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "gemini_chain = GraphCypherQAChain.from_llm(\n    ChatGoogleGenerativeAI(\n        model=\"gemini-pro\",\n        google_api_key=\"AIzaSyCaNF1Yh50y3TKwWZvxUJ6tqmrJ8x0FSuE\",\n    ),\n    graph=graph,\n    verbose=True,\n    cypher_prompt=prompt,\n)\ndef write_log_entry(entry, file_path):",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "file_path = \"data/benchmark/1_hop_500.json\"\n# Read the JSON file\nwith open(file_path, \"r\") as file:\n    data = json.load(file)\ndef call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_inference_times",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "gemini_inference_times = []\n# Process the first 20 samples\nfor i, x in enumerate(data[:20]):  # Limit to the first 20 samples\n    # Gemini inference\n    try:\n        start_time = time.time()\n        gemini_result = run(gemini_chain, x[\"question\"])\n        end_time = time.time()\n        gemini_inference_times.append(end_time - start_time)\n    except Exception as e:",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "average_gemini_inference_time",
        "kind": 5,
        "importPath": "experiments.RAG_gemini",
        "description": "experiments.RAG_gemini",
        "peekOfCode": "average_gemini_inference_time = sum(gemini_inference_times) / len(\n    gemini_inference_times\n)\nprint(f\"Average Inference Time for Gemini: {average_gemini_inference_time} seconds\")\n# Calculate mean scores and write to text files\ndef write_mean_scores(scores, file_path):\n    mean_scores = {\n        metric: sum(values) / len(values) for metric, values in scores.items()\n    }\n    with open(file_path, \"w\") as f:",
        "detail": "experiments.RAG_gemini",
        "documentation": {}
    },
    {
        "label": "get_authen",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def get_authen():\n    global access_token\n    if access_token != \"\":\n        return access_token\n    url = \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n    username = \"1bnt12o2j53b16id2eerklhd4a\"\n    password = \"kuo3n1nnkngel01lloems2ipop46ovt7l7fqdmb5mcmt5bv7ub2\"\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "get_apiKey",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def get_apiKey():\n    global access_token\n    global azure_api_key\n    if azure_api_key != \"\":\n        return azure_api_key\n    bearer_token = get_authen()\n    headers = {\n        \"Authorization\": f\"Bearer {bearer_token}\",\n        \"Content-Type\": \"application/json\",\n    }",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "get_azureGPT_model",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def get_azureGPT_model():\n    global azure_api_key\n    azure_api_key = get_apiKey()\n    llm = AzureChatOpenAI(\n        openai_api_key=azure_api_key,\n        openai_api_base=openai_api_base,\n        openai_api_type=openai_api_type,\n        openai_api_version=openai_api_version,\n        deployment_name=openai_api_deployment,\n        temperature=0,",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "get_scores",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n    return bleu, rouge_score, meteor\nexample_prompt = PromptTemplate.from_template(",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "write_log_entry",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def write_log_entry(entry, file_path):\n    with open(file_path, \"a\") as f:\n        f.write(json.dumps(entry, ensure_ascii=False, indent=4) + \",\\n\")\ndef run(chain, question):\n    return chain.run(question)\ndef call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def run(chain, question):\n    return chain.run(question)\ndef call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:\n            print(f\"Error: {e}. Retrying...\")\ndef get_azureGPT(text):",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "call_model_with_retry",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:\n            print(f\"Error: {e}. Retrying...\")\ndef get_azureGPT(text):\n    global azure_api_key\n    azure_api_key = get_apiKey()",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "get_azureGPT",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def get_azureGPT(text):\n    global azure_api_key\n    azure_api_key = get_apiKey()\n    llm = AzureChatOpenAI(\n        openai_api_key=azure_api_key,\n        openai_api_base=openai_api_base,\n        openai_api_type=openai_api_type,\n        openai_api_version=openai_api_version,\n        deployment_name=openai_api_deployment,\n        temperature=0,",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "call_model_with_retry",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:\n            print(f\"Error: {e}. Retrying...\")\nimport time\ngpt4_inference_times = []\n# Process the first 20 samples",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "write_mean_scores",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def write_mean_scores(scores, file_path):\n    mean_scores = {\n        metric: sum(values) / len(values) for metric, values in scores.items()\n    }\n    with open(file_path, \"w\") as f:\n        for metric, score in mean_scores.items():\n            f.write(f\"{metric}: {score}\\n\")\nwrite_mean_scores(gpt4_scores, gpt4_results_path)\n# Write logs to JSON files\ndef write_log(log, file_path):",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "write_log",
        "kind": 2,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "def write_log(log, file_path):\n    with open(file_path, \"w\") as f:\n        json.dump(log, f, ensure_ascii=False, indent=4)\nwrite_log(gpt4_log, gpt4_log_path)",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "access_token",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "access_token = \"\"\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\ncov_get_token_url = (\n    \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n)\ncov_get_apikey_url = \"https://api.covestro.com/openai/apikey\"\nopenai_api_base = os.getenv(\"OPENAI_API_BASE\")\nopenai_api_type = os.getenv(\"OPENAI_API_TYPE\")",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "cov_get_token_url",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "cov_get_token_url = (\n    \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n)\ncov_get_apikey_url = \"https://api.covestro.com/openai/apikey\"\nopenai_api_base = os.getenv(\"OPENAI_API_BASE\")\nopenai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\nazure_api_key = \"\"\nuri = os.getenv(\"URI\")",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "cov_get_apikey_url",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "cov_get_apikey_url = \"https://api.covestro.com/openai/apikey\"\nopenai_api_base = os.getenv(\"OPENAI_API_BASE\")\nopenai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\nazure_api_key = \"\"\nuri = os.getenv(\"URI\")\nuser = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "openai_api_base",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\nopenai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\nazure_api_key = \"\"\nuri = os.getenv(\"URI\")\nuser = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "openai_api_type",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\nazure_api_key = \"\"\nuri = os.getenv(\"URI\")\nuser = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():\n    global access_token",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "openai_api_version",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\nazure_api_key = \"\"\nuri = os.getenv(\"URI\")\nuser = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():\n    global access_token\n    if access_token != \"\":",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "openai_api_deployment",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "openai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\nazure_api_key = \"\"\nuri = os.getenv(\"URI\")\nuser = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():\n    global access_token\n    if access_token != \"\":\n        return access_token",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "azure_api_key",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "azure_api_key = \"\"\nuri = os.getenv(\"URI\")\nuser = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():\n    global access_token\n    if access_token != \"\":\n        return access_token\n    url = \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "uri",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "uri = os.getenv(\"URI\")\nuser = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():\n    global access_token\n    if access_token != \"\":\n        return access_token\n    url = \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n    username = \"1bnt12o2j53b16id2eerklhd4a\"",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "user",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "user = os.getenv(\"USER\")\npassword = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():\n    global access_token\n    if access_token != \"\":\n        return access_token\n    url = \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n    username = \"1bnt12o2j53b16id2eerklhd4a\"\n    password = \"kuo3n1nnkngel01lloems2ipop46ovt7l7fqdmb5mcmt5bv7ub2\"",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "password",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "password = os.getenv(\"PASSWORD\")\ngraph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():\n    global access_token\n    if access_token != \"\":\n        return access_token\n    url = \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n    username = \"1bnt12o2j53b16id2eerklhd4a\"\n    password = \"kuo3n1nnkngel01lloems2ipop46ovt7l7fqdmb5mcmt5bv7ub2\"\n    headers = {",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "graph",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "graph = Neo4jGraph(url=uri, username=user, password=password)\ndef get_authen():\n    global access_token\n    if access_token != \"\":\n        return access_token\n    url = \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n    username = \"1bnt12o2j53b16id2eerklhd4a\"\n    password = \"kuo3n1nnkngel01lloems2ipop46ovt7l7fqdmb5mcmt5bv7ub2\"\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "examples",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "examples = [\n    {\n        \"question\": \"Phương pháp điều trị cho bệnh [U lympho sau phúc mạc] là gì?\",\n        \"query\": \"MATCH (b:ĐIỀU_TRỊ) WHERE b.tên_bệnh = 'U lympho sau phúc mạc' RETURN b\",\n    },\n    {\n        \"question\": \"Nguyên nhân của bệnh [Chảy máu khoảng cách sau phúc mạc] là gì?\",\n        \"query\": \"MATCH (b:BỆNH) WHERE b.tên_bệnh = 'Chảy máu khoảng cách sau phúc mạc' RETURN b\",\n    },\n    {",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "rouge",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "rouge = Rouge()\n# Paths\nresults_dir = \"results\"\nlogs_dir = \"logs\"\ngpt4_results_path = os.path.join(results_dir, \"gpt4_graph_cypher.txt\")\ngpt4_log_path = os.path.join(logs_dir, \"gpt4_graph_cypher.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "results_dir",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "results_dir = \"results\"\nlogs_dir = \"logs\"\ngpt4_results_path = os.path.join(results_dir, \"gpt4_graph_cypher.txt\")\ngpt4_log_path = os.path.join(logs_dir, \"gpt4_graph_cypher.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "logs_dir",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "logs_dir = \"logs\"\ngpt4_results_path = os.path.join(results_dir, \"gpt4_graph_cypher.txt\")\ngpt4_log_path = os.path.join(logs_dir, \"gpt4_graph_cypher.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "gpt4_results_path",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "gpt4_results_path = os.path.join(results_dir, \"gpt4_graph_cypher.txt\")\ngpt4_log_path = os.path.join(logs_dir, \"gpt4_graph_cypher.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "gpt4_log_path",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "gpt4_log_path = os.path.join(logs_dir, \"gpt4_graph_cypher.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "gpt4_scores",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "gpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "gpt4_log",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "gpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "smoothing_function",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "smoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n    return bleu, rouge_score, meteor",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "example_prompt",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "example_prompt = PromptTemplate.from_template(\n    \"User input: {question}\\nCypher query: {query}\"\n)\nPREFIX = \"\"\"\n    I have a knowledge graph for Vietnamese traditional medicine, where each node represents a disease \"BỆNH\", \"ĐIỀU_TRỊ\", \"TRIỆU_CHỨNG\", \"THUỐC\", \"LỜI_KHUYÊN\". Each node can have the following properties:\n    1. BỆNH\n        - mô_tả_bệnh\n        - loại_bệnh\n        - tên_bệnh\n        - nguyên_nhân",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "PREFIX",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "PREFIX = \"\"\"\n    I have a knowledge graph for Vietnamese traditional medicine, where each node represents a disease \"BỆNH\", \"ĐIỀU_TRỊ\", \"TRIỆU_CHỨNG\", \"THUỐC\", \"LỜI_KHUYÊN\". Each node can have the following properties:\n    1. BỆNH\n        - mô_tả_bệnh\n        - loại_bệnh\n        - tên_bệnh\n        - nguyên_nhân\n    2. ĐIỀU_TRỊ\n        - khoa_điều_trị\n        - tỉ_lệ_chữa_khỏi",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "prompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=PREFIX,\n    suffix=\"User input: {question}\\nCypher query: \",\n    input_variables=[\"question\", \"schema\"],\n)\ngpt4_chain = GraphCypherQAChain.from_llm(\n    get_azureGPT_model(),\n    graph=graph,",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "gpt4_chain",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "gpt4_chain = GraphCypherQAChain.from_llm(\n    get_azureGPT_model(),\n    graph=graph,\n    verbose=True,\n    cypher_prompt=prompt,\n)\ndef write_log_entry(entry, file_path):\n    with open(file_path, \"a\") as f:\n        f.write(json.dumps(entry, ensure_ascii=False, indent=4) + \",\\n\")\ndef run(chain, question):",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "file_path = \"data/benchmark/1_hop_500.json\"\n# Read the JSON file\nwith open(file_path, \"r\") as file:\n    data = json.load(file)\ndef call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "gpt4_inference_times",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "gpt4_inference_times = []\n# Process the first 20 samples\nfor i, x in enumerate(data[:20]):  # Limit to the first 20 samples\n    # GPT-4 inference\n    try:\n        start_time = time.time()\n        gpt4_result = run(gpt4_chain, x[\"question\"])\n        end_time = time.time()\n        gpt4_inference_times.append(end_time - start_time)\n    except Exception as e:",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "average_gpt4_inference_time",
        "kind": 5,
        "importPath": "experiments.RAG_gpt",
        "description": "experiments.RAG_gpt",
        "peekOfCode": "average_gpt4_inference_time = sum(gpt4_inference_times) / len(gpt4_inference_times)\nprint(f\"Average Inference Time for GPT4: {average_gpt4_inference_time} seconds\")\n# Calculate mean scores and write to text files\ndef write_mean_scores(scores, file_path):\n    mean_scores = {\n        metric: sum(values) / len(values) for metric, values in scores.items()\n    }\n    with open(file_path, \"w\") as f:\n        for metric, score in mean_scores.items():\n            f.write(f\"{metric}: {score}\\n\")",
        "detail": "experiments.RAG_gpt",
        "documentation": {}
    },
    {
        "label": "requests_retry_session",
        "kind": 2,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "def requests_retry_session(\n    retries=5,\n    backoff_factor=1,\n    status_forcelist=(500, 502, 504),\n    session=None,\n):\n    session = session or requests.Session()\n    retry = Retry(\n        total=retries,\n        read=retries,",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "get_gemini",
        "kind": 2,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "def get_gemini(text):\n    response = llm.invoke([text])\n    # response = response.text\n    return response\n# Define the path to the JSON file\nfile_path = \"data/benchmark/resampled_1_hop.json\"\n# Read the JSON file\nwith open(file_path, \"r\") as file:\n    data = json.load(file)\nnltk.download(\"wordnet\")",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "get_scores",
        "kind": 2,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "def get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n    return bleu, rouge_score, meteor\ndef call_model_with_retry(model_func, prompt):",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "call_model_with_retry",
        "kind": 2,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "def call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:\n            print(f\"Error: {e}. Retrying...\")\nimport time\n# Initialize lists to store inference times\ngemini_inference_times = []",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "write_mean_scores",
        "kind": 2,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "def write_mean_scores(scores, file_path):\n    mean_scores = {\n        metric: sum(values) / len(values) for metric, values in scores.items()\n    }\n    with open(file_path, \"w\") as f:\n        for metric, score in mean_scores.items():\n            f.write(f\"{metric}: {score}\\n\")\nwrite_mean_scores(gemini_scores, gemini_results_path)\n# Write logs to JSON files\ndef write_log(log, file_path):",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "write_log",
        "kind": 2,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "def write_log(log, file_path):\n    with open(file_path, \"w\") as f:\n        json.dump(log, f, ensure_ascii=False, indent=4)\nwrite_log(gemini_log, gemini_log_path)",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "access_token",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "access_token = \"\"\n# Set up the model\ngeneration_config = {\n    \"temperature\": 0,\n    \"top_p\": 1,\n    \"top_k\": 1,\n    \"max_output_tokens\": 50000,\n}\nsafety_settings = [\n    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "generation_config",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "generation_config = {\n    \"temperature\": 0,\n    \"top_p\": 1,\n    \"top_k\": 1,\n    \"max_output_tokens\": 50000,\n}\nsafety_settings = [\n    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "safety_settings",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "safety_settings = [\n    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "google_api_key",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-pro\",\n    temperature=0.7,\n    top_p=0.85,\n    google_api_key=google_api_key,\n    convert_system_message_to_human=True,\n)\ndef get_gemini(text):\n    response = llm.invoke([text])",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "llm = ChatGoogleGenerativeAI(\n    model=\"gemini-pro\",\n    temperature=0.7,\n    top_p=0.85,\n    google_api_key=google_api_key,\n    convert_system_message_to_human=True,\n)\ndef get_gemini(text):\n    response = llm.invoke([text])\n    # response = response.text",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "file_path = \"data/benchmark/resampled_1_hop.json\"\n# Read the JSON file\nwith open(file_path, \"r\") as file:\n    data = json.load(file)\nnltk.download(\"wordnet\")\n# Initialize ROUGE scorer\nrouge = Rouge()\n# Paths\nresults_dir = \"results\"\nlogs_dir = \"logs\"",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "rouge",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "rouge = Rouge()\n# Paths\nresults_dir = \"results\"\nlogs_dir = \"logs\"\ngemini_results_path = os.path.join(results_dir, \"gemini_zero_shot.txt\")\ngemini_log_path = os.path.join(logs_dir, \"gemini_zero_shot.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "results_dir",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "results_dir = \"results\"\nlogs_dir = \"logs\"\ngemini_results_path = os.path.join(results_dir, \"gemini_zero_shot.txt\")\ngemini_log_path = os.path.join(logs_dir, \"gemini_zero_shot.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "logs_dir",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "logs_dir = \"logs\"\ngemini_results_path = os.path.join(results_dir, \"gemini_zero_shot.txt\")\ngemini_log_path = os.path.join(logs_dir, \"gemini_zero_shot.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_results_path",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "gemini_results_path = os.path.join(results_dir, \"gemini_zero_shot.txt\")\ngemini_log_path = os.path.join(logs_dir, \"gemini_zero_shot.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_log_path",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "gemini_log_path = os.path.join(logs_dir, \"gemini_zero_shot.json\")\n# Initialize lists to store scores\ngemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_scores",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "gemini_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_log",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "gemini_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "smoothing_function",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "smoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n    return bleu, rouge_score, meteor",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "gemini_inference_times",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "gemini_inference_times = []\n# Process the first 20 samples\nfor i, x in enumerate(data[:10]):  # Limit to the first 20 samples\n    PROMPT = f\"\"\"\n    Bạn là một chuyên gia về y học cổ truyền Việt Nam. Hãy trả lời đúng trọng tâm câu hỏi, không cần bổ sung thêm thông tin.\n    Câu hỏi: {x[\"question\"]}\n    \"\"\"\n    start_time = time.time()\n    gemini_result = call_model_with_retry(get_gemini, PROMPT)\n    end_time = time.time()",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "average_gemini_inference_time",
        "kind": 5,
        "importPath": "experiments.zero_shot_gemini",
        "description": "experiments.zero_shot_gemini",
        "peekOfCode": "average_gemini_inference_time = sum(gemini_inference_times) / len(\n    gemini_inference_times\n)\nprint(f\"Average Inference Time for Gemini: {average_gemini_inference_time} seconds\")\n# Calculate mean scores and write to text files\ndef write_mean_scores(scores, file_path):\n    mean_scores = {\n        metric: sum(values) / len(values) for metric, values in scores.items()\n    }\n    with open(file_path, \"w\") as f:",
        "detail": "experiments.zero_shot_gemini",
        "documentation": {}
    },
    {
        "label": "requests_retry_session",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def requests_retry_session(\n    retries=5,\n    backoff_factor=1,\n    status_forcelist=(500, 502, 504),\n    session=None,\n):\n    session = session or requests.Session()\n    retry = Retry(\n        total=retries,\n        read=retries,",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "get_authen",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def get_authen():\n    global access_token\n    if access_token != \"\":\n        return access_token\n    url = \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n    username = \"1bnt12o2j53b16id2eerklhd4a\"\n    password = \"kuo3n1nnkngel01lloems2ipop46ovt7l7fqdmb5mcmt5bv7ub2\"\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "get_covGPT",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def get_covGPT(text):\n    api_url = \"https://lengthy.api.covestro.com/openai/chat/gpt4\"\n    bearer_token = get_authen()\n    json_data = {\n        \"temperature\": 0,\n        # \"response_format\": {\"type\": \"json_object\"},\n        \"messages\": [\n            # {\"role\": \"system\", \"content\": \"You are a helpful assistant fluent in German, your task is to carefully read the given text and extract the information to the JSON format. You must return the output as the valid JSON, property name should be enclosed in double quotes.\"},\n            {\"role\": \"user\", \"content\": text}\n        ],",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "get_covGPT_section2",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def get_covGPT_section2(text):\n    api_url = \"https://lengthy.api.covestro.com/openai/chat/gpt4turbo\"\n    bearer_token = get_authen()\n    json_data = {\n        \"temperature\": 0,\n        \"messages\": [{\"role\": \"user\", \"content\": text}],\n        \"functions\": section2_custom_functions,\n        \"function_call\": \"auto\",\n    }\n    # Set headers with Bearer token",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "get_apiKey",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def get_apiKey():\n    global access_token\n    global azure_api_key\n    if azure_api_key != \"\":\n        return azure_api_key\n    bearer_token = get_authen()\n    headers = {\n        \"Authorization\": f\"Bearer {bearer_token}\",\n        \"Content-Type\": \"application/json\",\n    }",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "get_azureGPT",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def get_azureGPT(text):\n    global azure_api_key\n    azure_api_key = get_apiKey()\n    llm = AzureChatOpenAI(\n        openai_api_key=azure_api_key,\n        openai_api_base=openai_api_base,\n        openai_api_type=openai_api_type,\n        openai_api_version=openai_api_version,\n        deployment_name=openai_api_deployment,\n        temperature=0,",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "get_scores",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n    return bleu, rouge_score, meteor\ndef call_model_with_retry(model_func, prompt):",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "call_model_with_retry",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def call_model_with_retry(model_func, prompt):\n    while True:\n        try:\n            result = model_func(prompt)\n            return result\n        except Exception as e:\n            print(f\"Error: {e}. Retrying...\")\nimport time\n# Initialize lists to store inference times\ngpt4_inference_times = []",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "write_mean_scores",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def write_mean_scores(scores, file_path):\n    mean_scores = {\n        metric: sum(values) / len(values) for metric, values in scores.items()\n    }\n    with open(file_path, \"w\") as f:\n        for metric, score in mean_scores.items():\n            f.write(f\"{metric}: {score}\\n\")\nwrite_mean_scores(gpt4_scores, gpt4_results_path)\n# Write logs to JSON files\ndef write_log(log, file_path):",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "write_log",
        "kind": 2,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "def write_log(log, file_path):\n    with open(file_path, \"w\") as f:\n        json.dump(log, f, ensure_ascii=False, indent=4)\nwrite_log(gpt4_log, gpt4_log_path)",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "access_token",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "access_token = \"\"\ncov_get_token_url = (\n    \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n)\ncov_get_apikey_url = \"https://api.covestro.com/openai/apikey\"\nopenai_api_base = os.getenv(\"OPENAI_API_BASE\")\nopenai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\n# Set up the model",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "cov_get_token_url",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "cov_get_token_url = (\n    \"https://auth.api.covestro.com/oauth2/token?grant_type=client_credentials\"\n)\ncov_get_apikey_url = \"https://api.covestro.com/openai/apikey\"\nopenai_api_base = os.getenv(\"OPENAI_API_BASE\")\nopenai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\n# Set up the model\ngeneration_config = {",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "cov_get_apikey_url",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "cov_get_apikey_url = \"https://api.covestro.com/openai/apikey\"\nopenai_api_base = os.getenv(\"OPENAI_API_BASE\")\nopenai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\n# Set up the model\ngeneration_config = {\n    \"temperature\": 0,\n    \"top_p\": 1,\n    \"top_k\": 1,",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "openai_api_base",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\nopenai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\n# Set up the model\ngeneration_config = {\n    \"temperature\": 0,\n    \"top_p\": 1,\n    \"top_k\": 1,\n    \"max_output_tokens\": 50000,",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "openai_api_type",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\nopenai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\n# Set up the model\ngeneration_config = {\n    \"temperature\": 0,\n    \"top_p\": 1,\n    \"top_k\": 1,\n    \"max_output_tokens\": 50000,\n}",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "openai_api_version",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\nopenai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\n# Set up the model\ngeneration_config = {\n    \"temperature\": 0,\n    \"top_p\": 1,\n    \"top_k\": 1,\n    \"max_output_tokens\": 50000,\n}\nsafety_settings = [",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "openai_api_deployment",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "openai_api_deployment = os.getenv(\"OPENAI_MODEL_DEPLOYMENT\")\n# Set up the model\ngeneration_config = {\n    \"temperature\": 0,\n    \"top_p\": 1,\n    \"top_k\": 1,\n    \"max_output_tokens\": 50000,\n}\nsafety_settings = [\n    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "generation_config",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "generation_config = {\n    \"temperature\": 0,\n    \"top_p\": 1,\n    \"top_k\": 1,\n    \"max_output_tokens\": 50000,\n}\nsafety_settings = [\n    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "safety_settings",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "safety_settings = [\n    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "file_path = \"data/benchmark/resampled_1_hop.json\"\n# Read the JSON file\nwith open(file_path, \"r\") as file:\n    data = json.load(file)\nnltk.download(\"wordnet\")\n# Initialize ROUGE scorer\nrouge = Rouge()\n# Paths\nresults_dir = \"results\"\nlogs_dir = \"logs\"",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "rouge",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "rouge = Rouge()\n# Paths\nresults_dir = \"results\"\nlogs_dir = \"logs\"\ngpt4_results_path = os.path.join(results_dir, \"gpt4_zero_shot.txt\")\ngpt4_log_path = os.path.join(logs_dir, \"gpt4_zero_shot.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "results_dir",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "results_dir = \"results\"\nlogs_dir = \"logs\"\ngpt4_results_path = os.path.join(results_dir, \"gpt4_zero_shot.txt\")\ngpt4_log_path = os.path.join(logs_dir, \"gpt4_zero_shot.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "logs_dir",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "logs_dir = \"logs\"\ngpt4_results_path = os.path.join(results_dir, \"gpt4_zero_shot.txt\")\ngpt4_log_path = os.path.join(logs_dir, \"gpt4_zero_shot.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "gpt4_results_path",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "gpt4_results_path = os.path.join(results_dir, \"gpt4_zero_shot.txt\")\ngpt4_log_path = os.path.join(logs_dir, \"gpt4_zero_shot.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "gpt4_log_path",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "gpt4_log_path = os.path.join(logs_dir, \"gpt4_zero_shot.json\")\n# Initialize lists to store scores\ngpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "gpt4_scores",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "gpt4_scores = {\"BLEU\": [], \"ROUGE\": [], \"METEOR\": []}\n# Logs to store answers and scores\ngpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "gpt4_log",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "gpt4_log = []\nsmoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "smoothing_function",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "smoothing_function = SmoothingFunction().method1\ndef get_scores(hypothesis, reference):\n    hypothesis_tokens = hypothesis.split()\n    reference_tokens = reference.split()\n    bleu = sentence_bleu(\n        [reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function\n    )\n    rouge_score = rouge.get_scores(hypothesis, reference)[0][\"rouge-l\"][\"f\"]\n    meteor = meteor_score([reference_tokens], hypothesis_tokens)\n    return bleu, rouge_score, meteor",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "gpt4_inference_times",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "gpt4_inference_times = []\n# Process the first 20 samples\nfor i, x in enumerate(data[:10]):  # Limit to the first 20 samples\n    PROMPT = f\"\"\"\n    Bạn là một chuyên gia về y học cổ truyền Việt Nam. Hãy trả lời đúng trọng tâm câu hỏi, không cần bổ sung thêm thông tin.\n    Câu hỏi: {x[\"question\"]}\n    \"\"\"\n    # GPT-4 inference\n    start_time = time.time()\n    gpt4_result = call_model_with_retry(get_azureGPT, PROMPT)",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "average_gpt4_inference_time",
        "kind": 5,
        "importPath": "experiments.zero_shot_gpt4",
        "description": "experiments.zero_shot_gpt4",
        "peekOfCode": "average_gpt4_inference_time = sum(gpt4_inference_times) / len(gpt4_inference_times)\nprint(f\"Average Inference Time for GPT-4: {average_gpt4_inference_time} seconds\")\n# Calculate mean scores and write to text files\ndef write_mean_scores(scores, file_path):\n    mean_scores = {\n        metric: sum(values) / len(values) for metric, values in scores.items()\n    }\n    with open(file_path, \"w\") as f:\n        for metric, score in mean_scores.items():\n            f.write(f\"{metric}: {score}\\n\")",
        "detail": "experiments.zero_shot_gpt4",
        "documentation": {}
    },
    {
        "label": "create_list_of_dicts",
        "kind": 2,
        "importPath": "src.benchmark.create_triple",
        "description": "src.benchmark.create_triple",
        "peekOfCode": "def create_list_of_dicts(df):\n    result = []\n    for _, row in df.iterrows():\n        disease_name = row['disease_name']\n        for col in df.columns:\n            row[col] = str(row[col])\n            row[col] = row[col].replace(\"[\", \"\").replace(\"]\", \"\").replace('\\\"',\"\").replace(\"\\'\",\"\")\n            if col != 'disease_name' and (not pd.isna(row[col]) and row[col] != [] and row[col] != \"\" and row[col]!=\"nan\"):\n                result.append({\n                    \"header\": disease_name,",
        "detail": "src.benchmark.create_triple",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.benchmark.create_triple",
        "description": "src.benchmark.create_triple",
        "peekOfCode": "df = pd.read_csv(\"../../data/data_translated.csv\")\n# Create the list of dictionaries\nlist_of_dicts = create_list_of_dicts(df)\n# Save the list of dictionaries to a JSON file\noutput_file = '../../data/benchmark/output_1806.json'\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(list_of_dicts, f, ensure_ascii=False, indent=4)\nprint(f\"Data has been saved to {output_file}\")",
        "detail": "src.benchmark.create_triple",
        "documentation": {}
    },
    {
        "label": "list_of_dicts",
        "kind": 5,
        "importPath": "src.benchmark.create_triple",
        "description": "src.benchmark.create_triple",
        "peekOfCode": "list_of_dicts = create_list_of_dicts(df)\n# Save the list of dictionaries to a JSON file\noutput_file = '../../data/benchmark/output_1806.json'\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(list_of_dicts, f, ensure_ascii=False, indent=4)\nprint(f\"Data has been saved to {output_file}\")",
        "detail": "src.benchmark.create_triple",
        "documentation": {}
    },
    {
        "label": "output_file",
        "kind": 5,
        "importPath": "src.benchmark.create_triple",
        "description": "src.benchmark.create_triple",
        "peekOfCode": "output_file = '../../data/benchmark/output_1806.json'\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(list_of_dicts, f, ensure_ascii=False, indent=4)\nprint(f\"Data has been saved to {output_file}\")",
        "detail": "src.benchmark.create_triple",
        "documentation": {}
    },
    {
        "label": "uppercase_first_letter",
        "kind": 2,
        "importPath": "src.kgraph.create_KG",
        "description": "src.kgraph.create_KG",
        "peekOfCode": "def uppercase_first_letter(text):\n    if isinstance(text, str):\n        return text.capitalize()  # Capitalize first letter of each word\n    else:\n        return text\ndef clear_graph():\n    query = \"\"\"\n    MATCH (n)\n    DETACH DELETE n\n    \"\"\"",
        "detail": "src.kgraph.create_KG",
        "documentation": {}
    },
    {
        "label": "clear_graph",
        "kind": 2,
        "importPath": "src.kgraph.create_KG",
        "description": "src.kgraph.create_KG",
        "peekOfCode": "def clear_graph():\n    query = \"\"\"\n    MATCH (n)\n    DETACH DELETE n\n    \"\"\"\n    graph.run(query)\n    print(\"Graph has been cleared...\")\ndef check_node_exists(graph, associated_disease):\n    tên_bệnh = associated_disease.capitalize()\n    query = \"\"\"",
        "detail": "src.kgraph.create_KG",
        "documentation": {}
    },
    {
        "label": "check_node_exists",
        "kind": 2,
        "importPath": "src.kgraph.create_KG",
        "description": "src.kgraph.create_KG",
        "peekOfCode": "def check_node_exists(graph, associated_disease):\n    tên_bệnh = associated_disease.capitalize()\n    query = \"\"\"\n    MATCH (n:BỆNH {tên_bệnh: $tên_bệnh})\n    RETURN COUNT(n) > 0 AS node_exists\n    \"\"\"\n    result = graph.run(query, tên_bệnh=tên_bệnh).data()\n    return result[0][\"node_exists\"] if result else False\ndef process_row(row):\n    disease_name = row['disease_name']",
        "detail": "src.kgraph.create_KG",
        "documentation": {}
    },
    {
        "label": "process_row",
        "kind": 2,
        "importPath": "src.kgraph.create_KG",
        "description": "src.kgraph.create_KG",
        "peekOfCode": "def process_row(row):\n    disease_name = row['disease_name']\n    disease_description = row['disease_description']\n    disease_category = row['disease_category']\n    disease_prevention = row['disease_prevention']\n    disease_cause = row['disease_cause']\n    disease_symptom = row['disease_symptom']\n    people_easy_get = row['people_easy_get']\n    associated_disease = row['associated_disease']\n    cure_method = row['cure_method']",
        "detail": "src.kgraph.create_KG",
        "documentation": {}
    },
    {
        "label": "get_prompt",
        "kind": 2,
        "importPath": "src.kgraph.improve_translation",
        "description": "src.kgraph.improve_translation",
        "peekOfCode": "def get_prompt(json_input):\n    json_output = {\n        \"vietnamese_translation_adjusted\": \"NULL\"\n    }\n    prompt = f\"\"\"\n    As a highly skilled doctor and Vietnamese linguist, your task is to review and improve the quality of translated medical texts \n    from English to Vietnamese. You will receive input of Vietnamese translated version. Your job is to check the quality of translation for medical accuracy, trustworthy, linguistic correctness, \n    and overall readability. If necessary, make adjustments to ensure the Vietnamese translation is accurate and clear. If it's correct, return the origin text\n    Here is the json input: {json_input} and the output json should be in json format like this{json_output}, you must replace \"NULL\" by prompted text.\n    \"\"\"",
        "detail": "src.kgraph.improve_translation",
        "documentation": {}
    },
    {
        "label": "review_and_adjust_translation",
        "kind": 2,
        "importPath": "src.kgraph.improve_translation",
        "description": "src.kgraph.improve_translation",
        "peekOfCode": "def review_and_adjust_translation(vietnamese_medical_text):\n    # Simulated response from the language model\n    # In practice, this should call the language model API and process its response\n    json_input = {\"vietnamese_medical_text\": vietnamese_medical_text}\n    prompt = get_prompt(json_input)\n    # Here you should call the language model with the prompt and get the response\n    # For the sake of this example, we will assume the response from the language model is the input text\n    # Replace this with actual API call and processing logic\n    result = get_GPT(prompt)\n    try:",
        "detail": "src.kgraph.improve_translation",
        "documentation": {}
    },
    {
        "label": "save_df",
        "kind": 2,
        "importPath": "src.kgraph.improve_translation",
        "description": "src.kgraph.improve_translation",
        "peekOfCode": "def save_df(adjusted_df, file_path):\n    adjusted_df.to_csv(file_path, index=False)\n    print(f\"DataFrame saved to {file_path}\")\ndef create_adjusted_df(original_df, save_interval=2, file_path='adjusted_df.csv'):\n    adjusted_df = original_df.copy()\n    row_count = 0\n    for column in original_df.columns:\n        for index, value in original_df[column].items():\n            if pd.notnull(value):\n                adjusted_text = review_and_adjust_translation(value)",
        "detail": "src.kgraph.improve_translation",
        "documentation": {}
    },
    {
        "label": "create_adjusted_df",
        "kind": 2,
        "importPath": "src.kgraph.improve_translation",
        "description": "src.kgraph.improve_translation",
        "peekOfCode": "def create_adjusted_df(original_df, save_interval=2, file_path='adjusted_df.csv'):\n    adjusted_df = original_df.copy()\n    row_count = 0\n    for column in original_df.columns:\n        for index, value in original_df[column].items():\n            if pd.notnull(value):\n                adjusted_text = review_and_adjust_translation(value)\n                if adjusted_text != \"NULL\":\n                    adjusted_df.at[index, column] = adjusted_text\n            row_count += 1",
        "detail": "src.kgraph.improve_translation",
        "documentation": {}
    },
    {
        "label": "TextTranslator",
        "kind": 6,
        "importPath": "src.kgraph.translate",
        "description": "src.kgraph.translate",
        "peekOfCode": "class TextTranslator:\n    \"\"\"\n    A class to translate text from Chinese to Vietnamese using the Baidu translator.\n    \"\"\"\n    def __init__(self, professional_field=\"medicine\", sleep_seconds=5, limit_of_length=500000):\n        \"\"\"\n        Initialize the TextTranslator with specific translation settings.\n        Parameters:\n        professional_field (str): The field of expertise for translation (default is \"medicine\").\n        sleep_seconds (int): Sleep time between translations to avoid rate limits (default is 5 seconds).",
        "detail": "src.kgraph.translate",
        "documentation": {}
    },
    {
        "label": "CSVTranslator",
        "kind": 6,
        "importPath": "src.kgraph.translate",
        "description": "src.kgraph.translate",
        "peekOfCode": "class CSVTranslator:\n    \"\"\"\n    A class to handle translation of CSV files.\n    \"\"\"\n    def __init__(self, input_csv_file, output_csv_file, temp_output_csv_file, translator):\n        \"\"\"\n        Initialize the CSVTranslator with file paths and a translator instance.\n        Parameters:\n        input_csv_file (str): Path to the input CSV file.\n        output_csv_file (str): Path to the final output CSV file.",
        "detail": "src.kgraph.translate",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.kgraph.translate",
        "description": "src.kgraph.translate",
        "peekOfCode": "def main():\n    \"\"\"\n    Main function to execute the CSV translation.\n    \"\"\"\n    input_csv_file = \"../data/raw_data.csv\"\n    output_csv_file = \"data_translated.csv\"\n    temp_output_csv_file = \"data_temp.csv\"\n    translator = TextTranslator()\n    csv_translator = CSVTranslator(input_csv_file, output_csv_file, temp_output_csv_file, translator)\n    csv_translator.translate_csv()",
        "detail": "src.kgraph.translate",
        "documentation": {}
    },
    {
        "label": "get_GPT",
        "kind": 2,
        "importPath": "src.llm",
        "description": "src.llm",
        "peekOfCode": "def get_GPT(text):\n    os.environ['OPENAI_API_KEY'] = api_key\n    openai.api_key = os.environ['OPENAI_API_KEY']\n    client = OpenAI()\n    response = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\n        \"role\": \"system\", \"content\": \"You are a Vietnamese and Chinease linguistic and pharmarcist and expert in 2 these fields\",\n        \"role\": \"user\", \"content\": text}],\n    temperature=0,",
        "detail": "src.llm",
        "documentation": {}
    },
    {
        "label": "get_gemini",
        "kind": 2,
        "importPath": "src.llm",
        "description": "src.llm",
        "peekOfCode": "def get_gemini(text): \n    response = model.generate_content([text])\n    response = response.text\n    return response",
        "detail": "src.llm",
        "documentation": {}
    },
    {
        "label": "generation_config",
        "kind": 5,
        "importPath": "src.llm",
        "description": "src.llm",
        "peekOfCode": "generation_config = {\n  \"temperature\": 0,\n  \"top_p\": 1,\n  \"top_k\": 1,\n  \"max_output_tokens\": 50000,\n}\nsafety_settings = [\n  {\n    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"",
        "detail": "src.llm",
        "documentation": {}
    },
    {
        "label": "safety_settings",
        "kind": 5,
        "importPath": "src.llm",
        "description": "src.llm",
        "peekOfCode": "safety_settings = [\n  {\n    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n  },\n  {",
        "detail": "src.llm",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "src.llm",
        "description": "src.llm",
        "peekOfCode": "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro-latest\",\n                              generation_config=generation_config,\n                              safety_settings=safety_settings)\ndef get_GPT(text):\n    os.environ['OPENAI_API_KEY'] = api_key\n    openai.api_key = os.environ['OPENAI_API_KEY']\n    client = OpenAI()\n    response = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{",
        "detail": "src.llm",
        "documentation": {}
    }
]